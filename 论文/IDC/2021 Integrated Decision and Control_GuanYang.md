
### 算法一
![[算法一.png]]
### 例子：自动驾驶汽车的变道任务
假设我们有一辆自动驾驶汽车在高速公路上行驶，需要学习如何在不同交通状况下变道。目标是训练一个RL模型，让汽车能够在不同的车流条件下选择合适的控制动作（加速、减速、向左转、向右转等），以安全有效地完成变道。

#### 状态
在这个例子中，状态 \( S \) 包括：
- 汽车当前的速度和位置。
- 邻近车道内其他车辆的位置和速度。
- 目标车道的距离。
- 车辆周围的空隙大小（即变道的空间）。

#### 动作
控制动作 \( A \) 是汽车可以选择的操作，例如：
- 加速或减速。
- 向左或向右转向。
- 保持当前车道不变。

#### 策略的作用
在传统的控制优化中，每一次变道决策都需要计算出准确的加速、减速、或转向的数值来实现最优控制。而在RL中，策略（policy）则学习如何根据当前的状态，直接选择一个适当的动作。

##### 策略的具体含义
假设这个策略是一个神经网络，输入状态数据（汽车和周围车辆的位置信息等），输出控制动作。例如：
- 如果检测到目标车道后方有快速接近的车辆，策略可能选择 **等待保持车道**。
- 如果邻近车道空间足够，且目标车道车辆速度适中，策略可能选择 **右转转向并加速**，从而完成变道。

在这种情况下，策略不是直接告诉汽车以多大的加速度变道，而是告诉它“在这个状态下，什么样的动作最合适”。一旦策略训练完成，汽车就可以在每次需要变道时，直接根据当前状态来查找策略，得到合适的动作，而不是每次都重新计算最优控制输入。这种策略大大提高了计算效率，因为它将变道决策从连续的优化计算中解耦出来，转化为一个映射关系的查找过程。

---
### 算法 2 
是用于动态最优跟踪的离线训练算法，具体分为采样和优化两个步骤。

#### 1. 初始化
- 初始化评论器网络 $V_w$ 和演员网络 $\pi_\theta$，其参数分别为 $w$ 和 $\theta$。
- 创建经验缓冲区 $B$，用于存储采样数据。
- 设置学习率 $\beta_w$ 和 $\beta_\theta$，惩罚因子 $\rho$ 初始为 1，并设置惩罚放大器 $c$ 和更新间隔 $m$。

#### 2. 采样（Sampling）
- 选择一个候选路径 $\tau$，并初始化自车状态 $x_t$ 和其他车辆状态 $x^j_t$。
- 在每个环境步中：
  - 构造当前状态 $s_t$，包括候选路径、自车和周围车辆的信息。
  - 将状态 $s_t$ 存入缓冲区 $B$。
  - 使用演员网络 $\pi_\theta$ 来输出控制动作 $u_t$。
  - 应用 $u_t$ 后，观察下一时刻的自车状态 $x_{t+1}$ 和其他车辆状态 $x^j_{t+1}$。

#### 3. 优化（Optimizing）
- 从缓冲区 $B$ 中提取一批状态，计算评论器和演员的损失 $J_{critic}$ 和 $J_p$。
- 评论器更新（Policy Evaluation - PEV）：使用梯度下降法更新评论器的参数 $w$。
- 演员更新（Policy Improvement - PIM）：如果当前迭代次数 $i$ 是更新间隔 $m$ 的倍数，增加惩罚因子 $\rho$，并使用 $\beta_\theta$ 更新演员的参数 $\theta$ 以最小化损失函数 $J_p$。

#### 4. 假设（Assumption 1）
- 假设指出，随着神经网络逐渐收敛，优化过程中的解与最优解之间的差距逐步减小，即 $\Delta_k \rightarrow 0$。
- 基于该假设，算法通过不断优化来减小损失，使得获得的策略能在不同场景下有效地执行路径跟踪。

该算法的核心是离线优化策略，以便在实际应用中能够快速、实时地选择和跟踪最优路径。


### 算法3 : Dynamic Optimal Tracking - Online Application

**Initialize**: 从上层模块获取路径集合 \( \Pi \)，加载训练好的评论器网络 \( V_{w_*} \) 和演员网络 \( \pi_{\theta_*} \)，以及自车状态 \( x_t \) 和周围车辆的状态 \( x_t^j \)，其中 \( j \in I \)。

**For each environment step do**:
1. **Selecting**:
   - 对于每条候选路径 \( \tau \in \Pi \):
     - 构造当前状态 \( s_{t, \tau} = \{ \tau, x_t, x_t^j | j \in I \} \)。
     - 计算路径的价值 \( V^*_\tau = V_{w_*}(s_{t, \tau}) \)。
   - 选择具有最低价值的路径 \( \tau^* \):
     \[
     \tau^* = \arg \min_\tau \{ V^*_\tau | \tau \in \Pi \}
     \]

2. **Tracking**:
   - 构造当前状态 \( s_t = \{ \tau^*, x_t, x_t^j | j \in I \} \)。
   - 使用演员网络 \( \pi_{\theta_*} \) 计算控制动作 \( u_t^* = \pi_{\theta_*}(s_t) \)。
   - 使用公式 (24) 计算安全控制动作 \( u_t^{safe} \)：
     \[
     u_t^{safe} = \begin{cases} 
     u_t^*, & \text{if } u_t^* \in U_{safe}(s_t) \\
     \arg \min_{u \in U_{safe}(s_t)} \| u - u_t^* \|^2, & \text{else} 
     \end{cases}
     \]
   - 应用 \( u_t^{safe} \) 来更新自车和周围车辆的状态，观察 \( x_{t+1} \) 和 \( x_{t+1}^j, j \in I \)。

**End for**.

---

注释：
- 该算法包含两个主要步骤：路径选择和路径跟踪。在路径选择步骤中，算法从候选路径集合 \( \Pi \) 中选择价值最低的路径。在路径跟踪步骤中，使用演员网络计算控制动作，并通过安全屏障来确保控制动作在安全范围内。

### 异步学习架构 (Asynchronous Learning Architecture)
异步学习架构是一种并行化的机器学习方法。在传统的同步学习架构中，多个计算线程（或代理）需要等待彼此完成操作，然后统一更新模型参数。然而，异步学习架构允许每个线程独立地探索环境、收集数据并更新模型，而无需等待其他线程的结果。这样可以更有效地利用计算资源，加速学习过程。

在这种架构中，每个线程可以在不同的环境中与主模型进行交互并执行更新。每个线程计算其自身的梯度，然后异步地累积这些梯度并更新主模型。这样不仅加快了收敛速度，还可以增加模型的鲁棒性，因为不同线程在不同环境中采样的数据可以增强模型的泛化能力。

典型的异步算法之一是 **A3C（Asynchronous Advantage Actor-Critic）**，它在强化学习中非常流行。A3C 中多个线程独立地与环境进行交互，并异步更新共享模型参数。这种方法特别适合于需要在大规模数据或复杂环境中进行训练的场景。

### Adam 优化算法
Adam（Adaptive Moment Estimation）是一种基于梯度的一阶优化算法，专门用于神经网络的权重更新。Adam 算法结合了 **动量方法（Momentum）** 和 **RMSProp** 的优点。具体来说，Adam 算法通过计算梯度的一阶和二阶矩的移动平均值来调整每个参数的学习率。

Adam 的核心更新公式如下：
- 假设梯度为 $g_t$，则一阶动量（梯度的移动平均）和二阶动量（梯度平方的移动平均）计算为：
	$$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$$
  $$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$$

- 然后使用这些动量来更新参数：
  $$\theta_t = \theta_{t-1} - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$$

  其中 $\hat{m}_t$ 和 $\hat{v}_t$ 是经过偏差修正的动量项。

与传统的梯度下降相比，Adam 通过动态调整学习率使训练更稳定且更快速。由于 Adam 能有效处理稀疏梯度和非平稳的优化目标，因此在深度学习任务中非常流行。

### 多层感知机（MLP）作为价值函数和策略的模型
多层感知机（MLP）是一种前馈神经网络，包含输入层、多个隐藏层和输出层。每层神经元与下一层神经元之间是全连接的。MLP 可以用来近似复杂的函数，是一种广泛使用的非线性模型。

在强化学习中，MLP 可以用来表示 **价值函数** 和 **策略函数**：
- **价值函数**：估计给定状态或状态-动作对的预期回报。比如在 Q-learning 中，MLP 可用来估计状态-动作对的 Q 值。
- **策略函数**：直接输出在特定状态下的最优动作，通常用于策略梯度方法（如 Actor-Critic 算法）。

MLP 的计算公式如下：
- 每层的输出是前一层输出的线性组合，再经过激活函数：
  $h = \text{Activation}(W \cdot x + b)$
  其中 $W$ 是权重矩阵， $b$是偏置项，Activation 是激活函数。
---
### 多层感知机（MLP）与神经网络的联系与区别
**多层感知机（MLP）** 是一种前馈神经网络，是 **神经网络** 的一个特定类型。两者之间的关系可以理解为：MLP 是神经网络的一种特定形式，但并不是所有神经网络都是 MLP。

#### 2.1 多层感知机（MLP）
- MLP 是一种 **前馈神经网络**，包含输入层、一个或多个隐藏层和输出层。每一层中的神经元（单元）仅连接到前一层和下一层的神经元，层与层之间是全连接的。
- MLP 通常用于处理 **结构化数据**，如分类、回归等任务。其典型架构如下：
  - **输入层**：接收输入数据。
  - **隐藏层**：通过加权和偏置组合前一层的输出，经过激活函数进行非线性变换。
  - **输出层**：输出结果，用于预测或分类。
- 公式表示：给定输入 \( x \)，每一层的输出计算如下：
$$
  h = \sigma(W \cdot x + b)
  $$
  其中 $W$ 是权重矩阵， $b$ 是偏置， $\sigma$ 是激活函数（如 ReLU、Sigmoid 等）。

#### 2.2 神经网络
- **神经网络** 是一个更广泛的术语，涵盖了多种结构，如 MLP、卷积神经网络（CNN）、循环神经网络（RNN）等。
- 神经网络可以有不同的拓扑结构和层次组织形式，以适应不同任务需求：
  - **卷积神经网络（CNN）**：用于图像处理，通过卷积层提取局部特征。
  - **循环神经网络（RNN）**：用于处理序列数据，具有时间依赖性。
  - **生成对抗网络（GAN）**：用于生成新数据，由生成器和判别器组成。
- 神经网络不仅限于前馈结构，还可以有反馈、循环连接等复杂结构，适用于图像识别、自然语言处理、生成任务等。

#### 联系与区别
- **联系**：
  - MLP 是神经网络的一种特定类型，即前馈神经网络。
  - 它们都基于神经元的激活和权重更新，通过梯度下降等算法进行训练。
  - 都可以通过层叠组合多个神经元层，从输入到输出进行非线性映射。

- **区别**：
  - MLP 是特定于结构简单、全连接的神经网络。它仅用于静态数据，通常不适合处理时序或空间相关数据。
  - 神经网络是一个更广泛的概念，包含不同的结构以适应各种数据类型，如 CNN 可以处理图像数据，RNN 适合时间序列。
  - 神经网络包括多种层类型和连接方式，而 MLP 通常是全连接的且只有前向传播。

简而言之，MLP 是一种特定形式的神经网络，适用于简单的分类和回归任务，而神经网络这个概念则包含了更广泛的模型，能够适应不同类型和更复杂的数据结构。




### 指数线性单元（ELU）激活函数
ELU（Exponential Linear Unit）是一种激活函数，与 ReLU 类似，但在 \( x \leq 0 \) 时，它会输出负值以减小偏移，使得激活函数的输出均值更接近于零。其定义为：
  
 $$ f(x) = \begin{cases}
  x & \text{if } x > 0 \\
  \alpha (\exp(x) - 1) & \text{if } x \leq 0
  \end{cases}$$
  
其中 $\alpha$ 是一个超参数，通常设为 1。

与 ReLU 相比，ELU 在 $x < 0$  时是平滑的负值，使得网络收敛速度更快。它可以避免 ReLU 的 **死神经元问题**（即神经元输出始终为零），并在反向传播时保持较大的梯度，减小梯度消失的风险。

### 多项式衰减学习率
多项式衰减是一种动态调整学习率的方法，训练时学习率随着迭代次数逐渐减小，通常采用以下公式：
  $$\text{learning rate} = \text{initial learning rate} \times \left(1 - \frac{t}{T}\right)^p$$
  其中 $t$ 是当前步数，$T$ 是总步数，$p$ 是多项式指数控制衰减速度。

在训练早期使用较高的学习率可以帮助模型快速收敛，而在后期逐渐减小学习率有助于模型更精细地搜索最佳解，从而提升最终的模型性能。

---
### Q-learning
Q-learning 是一种 **基于值的强化学习算法**，用于学习代理（Agent）在不同状态下采取不同动作的最优策略。它基于 **Q值** 的更新过程，通过与环境的交互，代理学习如何在每个状态中采取使长期回报最大的动作。

#### 工作原理
- 在 Q-learning 中，代理学习一个 **Q函数**，表示在给定状态 \( s \) 下执行某动作 \( a \) 的预期回报。这个函数通常表示为 \( Q(s, a) \)。
- 每次代理采取一个动作并观察到奖励和新的状态后，就会更新 Q 值。更新公式如下：
  $$
  Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
  $$
  - \( \alpha \)：学习率，控制每次更新的幅度。
  - \( \gamma \)：折扣因子，表示未来奖励的重要性。
  - \( r \)：当前动作 \( a \) 在状态 \( s \) 下获得的即时奖励。
  - \( s' \)：代理执行动作 \( a \) 后转移到的新状态。
  - \( \max_{a'} Q(s', a') \)：在新状态 \( s' \) 下，选择能产生最大预期回报的动作。

#### 特点
- **无模型方法**：Q-learning 是一种无模型的算法，即不需要知道环境的转移概率，直接从与环境的交互中学习。
- **离线学习**：代理可以在没有模型的情况下使用 Q-learning 来生成策略并在离线更新。
- **适用于离散动作空间**：Q-learning 通常用于离散动作空间中，对于连续动作空间需要其他方法如 DDPG 或 DQN 的拓展。

**超参数（Hyperparameters）** 是在机器学习和深度学习模型训练过程中事先设置的参数，不同于模型内部在训练过程中通过数据自动学习到的 **参数（parameters）**。超参数的值直接影响模型训练的效率和最终性能，但它们不会随着训练数据的变化而自动更新。为了获得最佳性能，需要在训练前或训练过程中通过实验调整超参数。

### 超参数的类型
超参数可以分为以下几类：

1. **模型结构超参数**：
   - 决定模型的结构和复杂性。例如：
     - **神经网络中的层数和每层的神经元数**：更多层或更多神经元数可以增加模型的容量，使其能够拟合更复杂的数据。
     - **卷积神经网络中的卷积核大小和数量**：影响特征提取的粒度和特征空间的丰富程度。
     - **激活函数的选择**：决定每层输出的非线性变换形式，如 ReLU、Sigmoid、Tanh 等。

2. **优化超参数**：
   - 控制模型训练过程中的优化算法行为。例如：
     - **学习率（Learning Rate）**：控制每次梯度更新的步长。如果学习率太高，模型可能会跳过最优解；如果太低，模型收敛会很慢。
     - **批量大小（Batch Size）**：每次更新权重时使用的数据样本数量。较大的批量大小可以稳定训练过程，但需要更多内存；较小的批量大小则可以更快更新。
     - **优化算法的选择**：不同优化算法（如 SGD、Adam、RMSprop 等）会影响梯度下降的方式和速度。

3. **正则化超参数**：
   - 控制模型的正则化力度，以防止过拟合。例如：
     - **L2 正则化系数**：惩罚权重较大的参数，限制模型复杂度。
     - **Dropout 比例**：在神经网络的训练过程中随机忽略一些神经元，以降低过拟合。
     - **早停（Early Stopping）**：通过在验证集上的性能来控制训练轮数，从而防止训练过度。

4. **学习率调度超参数**：
   - 调整学习率在训练过程中的变化。例如：
     - **学习率衰减率**：设置学习率如何随时间逐步减小，常见的衰减策略包括指数衰减、分段下降、余弦衰减等。
     - **衰减步长**：控制学习率下降的频率。

### 超参数的选择
超参数通常需要通过实验调优。调优超参数的方法包括：
- **网格搜索（Grid Search）**：在一定范围内选择多个超参数的组合进行训练，逐一尝试所有可能的组合。
- **随机搜索（Random Search）**：随机采样超参数组合，可以探索更大的超参数空间。
- **贝叶斯优化（Bayesian Optimization）**：基于先验知识和训练数据进行超参数调优，以优化模型性能。
- **AutoML 方法**：使用机器学习算法自动搜索最优超参数，如自动化超参数调优库（如 Optuna、Hyperopt 等）。

### 超参数与参数的区别
- **参数**：模型在训练过程中自动学习到的值，例如神经网络中的权重和偏置项。这些参数通过梯度下降等优化算法不断调整，最终达到最优解。
- **超参数**：事先设定的外部配置，在训练过程中不变。模型通过不同的超参数设置来调整训练的速率、正则化程度、模型复杂度等，以寻找最佳性能。

总的来说，超参数的选择对模型的性能有重大影响。优化超参数的过程通常需要实验和经验，并且对于不同的问题和数据集，超参数的最佳配置可能会有所不同。

(https://youtu.be/J8aRgcjJukQ)
